{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31df3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from tqdm import notebook\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119c8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f32a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    dataset_dir = \".data\",\n",
    "    dataset_prefix = \"faq_with_splits_\",\n",
    "    datasets = [\"tokenized\", \"filtered\", \"lemmatized\", \"stemmed\", \"lemmatized_filtered\", \"stemmed_filtered\"],\n",
    "    model_save_dir = \".model_storage/MLP\",\n",
    "    model_state_file = \"model\",\n",
    "    seed = 1234,\n",
    "    num_epochs = 5,\n",
    "    learning_rate = 1e-3,\n",
    "    hidden_size = 100,\n",
    "    batch_size = 128,\n",
    "    cuda = True,\n",
    "    train_column = 'short_question'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5733a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if args.cuda & torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20422b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_state_file = os.path.join(args.model_save_dir, args.model_state_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78648d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".model_storage/MLP directory already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.model_save_dir):\n",
    "    os.makedirs(args.model_save_dir)\n",
    "    print(f\"Created directory {args.model_save_dir}\")\n",
    "else:\n",
    "    print(f\"{args.model_save_dir} directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548342c9",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3218cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file: .data\\faq_with_splits_tokenized.csv\n",
      "Opening file: .data\\faq_with_splits_filtered.csv\n",
      "Opening file: .data\\faq_with_splits_lemmatized.csv\n",
      "Opening file: .data\\faq_with_splits_stemmed.csv\n",
      "Opening file: .data\\faq_with_splits_lemmatized_filtered.csv\n",
      "Opening file: .data\\faq_with_splits_stemmed_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "faq_dataframes = {}\n",
    "\n",
    "for dataset in args.datasets:\n",
    "    df_path = os.path.join(args.dataset_dir, args.dataset_prefix + dataset + \".csv\")\n",
    "    print(f\"Opening file: {df_path}\")\n",
    "    faq_dataframes[dataset] = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2632cc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_question</th>\n",
       "      <th>long_question</th>\n",
       "      <th>answer</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hogy tudom a hamis bevésődéseket rossz gondola...</td>\n",
       "      <td>Hogy tudom a hamis bevésődéseket rossz gondola...</td>\n",
       "      <td>Korlátozó hiedelmeknek hívják őket, de tudom, ...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Mentális egészség</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "      <td>Röviden: Dancsó lehúzta Fankadeli egyik haverj...</td>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Sztárok, bulvár</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "      <td>A pontozás pont olyan, mint a dominionnál volt...</td>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Játékok</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A legjobb fogorvos Magyarországon</td>\n",
       "      <td>A legjobb fogorvos Magyarországon Szóval tudni...</td>\n",
       "      <td>Nincs olyan, hogy legjobb, a fogorvosok ugyanú...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Fogak, szájápolás</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "      <td>Amúgy tényleg, szinte minden nap kiír valami f...</td>\n",
       "      <td>Állatok</td>\n",
       "      <td>Kisemlősök</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      short_question  \\\n",
       "0  Hogy tudom a hamis bevésődéseket rossz gondola...   \n",
       "1  Milyen balhé volt Dancsó Péter és Fankadeli kö...   \n",
       "2  Hol vannak leírva a Semmiképp sem Dominion sza...   \n",
       "3                  A legjobb fogorvos Magyarországon   \n",
       "4  A hörcsögök egész szeme fekete vagy csak egy r...   \n",
       "\n",
       "                                       long_question  \\\n",
       "0  Hogy tudom a hamis bevésődéseket rossz gondola...   \n",
       "1  Milyen balhé volt Dancsó Péter és Fankadeli kö...   \n",
       "2  Hol vannak leírva a Semmiképp sem Dominion sza...   \n",
       "3  A legjobb fogorvos Magyarországon Szóval tudni...   \n",
       "4  A hörcsögök egész szeme fekete vagy csak egy r...   \n",
       "\n",
       "                                              answer main_category  \\\n",
       "0  Korlátozó hiedelmeknek hívják őket, de tudom, ...      Egészség   \n",
       "1  Röviden: Dancsó lehúzta Fankadeli egyik haverj...    Szórakozás   \n",
       "2  A pontozás pont olyan, mint a dominionnál volt...    Szórakozás   \n",
       "3  Nincs olyan, hogy legjobb, a fogorvosok ugyanú...      Egészség   \n",
       "4  Amúgy tényleg, szinte minden nap kiír valami f...       Állatok   \n",
       "\n",
       "        sub_category  split  \n",
       "0  Mentális egészség    val  \n",
       "1    Sztárok, bulvár  train  \n",
       "2            Játékok  train  \n",
       "3  Fogak, szájápolás  train  \n",
       "4         Kisemlősök  train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_dataframes[args.datasets[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4428048",
   "metadata": {},
   "source": [
    "## Seperate datasets by splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b262a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes_for_split(split, datasets):\n",
    "    return {ds: datasets[ds][datasets[ds].split == split] for ds in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc6b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = get_dataframes_for_split('train', faq_dataframes)\n",
    "test_dfs = get_dataframes_for_split('test', faq_dataframes)\n",
    "valid_dfs = get_dataframes_for_split('val', faq_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ad6b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_question</th>\n",
       "      <th>long_question</th>\n",
       "      <th>answer</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "      <td>Röviden: Dancsó lehúzta Fankadeli egyik haverj...</td>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Sztárok, bulvár</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "      <td>A pontozás pont olyan, mint a dominionnál volt...</td>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Játékok</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A legjobb fogorvos Magyarországon</td>\n",
       "      <td>A legjobb fogorvos Magyarországon Szóval tudni...</td>\n",
       "      <td>Nincs olyan, hogy legjobb, a fogorvosok ugyanú...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Fogak, szájápolás</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "      <td>Amúgy tényleg, szinte minden nap kiír valami f...</td>\n",
       "      <td>Állatok</td>\n",
       "      <td>Kisemlősök</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mi történt a Sonic Boom al a Megamax on</td>\n",
       "      <td>Mi történt a Sonic Boom al a Megamax on Egy na...</td>\n",
       "      <td>Chara the FirstHuman nevű felhasználó válasza:...</td>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Filmek, sorozatok</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      short_question  \\\n",
       "1  Milyen balhé volt Dancsó Péter és Fankadeli kö...   \n",
       "2  Hol vannak leírva a Semmiképp sem Dominion sza...   \n",
       "3                  A legjobb fogorvos Magyarországon   \n",
       "4  A hörcsögök egész szeme fekete vagy csak egy r...   \n",
       "5            Mi történt a Sonic Boom al a Megamax on   \n",
       "\n",
       "                                       long_question  \\\n",
       "1  Milyen balhé volt Dancsó Péter és Fankadeli kö...   \n",
       "2  Hol vannak leírva a Semmiképp sem Dominion sza...   \n",
       "3  A legjobb fogorvos Magyarországon Szóval tudni...   \n",
       "4  A hörcsögök egész szeme fekete vagy csak egy r...   \n",
       "5  Mi történt a Sonic Boom al a Megamax on Egy na...   \n",
       "\n",
       "                                              answer main_category  \\\n",
       "1  Röviden: Dancsó lehúzta Fankadeli egyik haverj...    Szórakozás   \n",
       "2  A pontozás pont olyan, mint a dominionnál volt...    Szórakozás   \n",
       "3  Nincs olyan, hogy legjobb, a fogorvosok ugyanú...      Egészség   \n",
       "4  Amúgy tényleg, szinte minden nap kiír valami f...       Állatok   \n",
       "5  Chara the FirstHuman nevű felhasználó válasza:...    Szórakozás   \n",
       "\n",
       "        sub_category  split  \n",
       "1    Sztárok, bulvár  train  \n",
       "2            Játékok  train  \n",
       "3  Fogak, szájápolás  train  \n",
       "4         Kisemlősök  train  \n",
       "5  Filmek, sorozatok  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dfs[args.datasets[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb311d",
   "metadata": {},
   "source": [
    "## Drop the unnecessary columns\n",
    "Keeping the following columns:\n",
    "- main category\n",
    "- short question\n",
    "- long question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2f0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_columns(datasets, columns):\n",
    "    return {ds: datasets[ds][columns] for ds in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3933dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['main_category', 'short_question', 'long_question']\n",
    "\n",
    "train_dfs = keep_columns(train_dfs, columns)\n",
    "test_dfs = keep_columns(test_dfs, columns)\n",
    "valid_dfs = keep_columns(valid_dfs, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b040e6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>short_question</th>\n",
       "      <th>long_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "      <td>Milyen balhé volt Dancsó Péter és Fankadeli kö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "      <td>Hol vannak leírva a Semmiképp sem Dominion sza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egészség</td>\n",
       "      <td>A legjobb fogorvos Magyarországon</td>\n",
       "      <td>A legjobb fogorvos Magyarországon Szóval tudni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Állatok</td>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "      <td>A hörcsögök egész szeme fekete vagy csak egy r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Szórakozás</td>\n",
       "      <td>Mi történt a Sonic Boom al a Megamax on</td>\n",
       "      <td>Mi történt a Sonic Boom al a Megamax on Egy na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category                                     short_question  \\\n",
       "1    Szórakozás  Milyen balhé volt Dancsó Péter és Fankadeli kö...   \n",
       "2    Szórakozás  Hol vannak leírva a Semmiképp sem Dominion sza...   \n",
       "3      Egészség                  A legjobb fogorvos Magyarországon   \n",
       "4       Állatok  A hörcsögök egész szeme fekete vagy csak egy r...   \n",
       "5    Szórakozás            Mi történt a Sonic Boom al a Megamax on   \n",
       "\n",
       "                                       long_question  \n",
       "1  Milyen balhé volt Dancsó Péter és Fankadeli kö...  \n",
       "2  Hol vannak leírva a Semmiképp sem Dominion sza...  \n",
       "3  A legjobb fogorvos Magyarországon Szóval tudni...  \n",
       "4  A hörcsögök egész szeme fekete vagy csak egy r...  \n",
       "5  Mi történt a Sonic Boom al a Megamax on Egy na...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dfs[args.datasets[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0241e",
   "metadata": {},
   "source": [
    "#### Get the main categories\n",
    "Get main category names and assign ids for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b221180",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = faq_dataframes[args.datasets[0]].main_category.unique().tolist()\n",
    "target_dict = {k: v for v, k in enumerate(target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6104718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Egészség', 'Szórakozás', 'Állatok', 'Számítástechnika']\n",
      "{'Egészség': 0, 'Szórakozás': 1, 'Állatok': 2, 'Számítástechnika': 3}\n"
     ]
    }
   ],
   "source": [
    "print(target_names)\n",
    "print(target_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8d04d",
   "metadata": {},
   "source": [
    "### Get vocabulary for bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeac987",
   "metadata": {},
   "source": [
    "Which way of getting the vocabulary is faster? The primitive approach, iterating over the dataframe row by row, or the sigma male approach, which takes advantage of the library’s potential. This is a rhetorical question. It could be faster imo, but I'm pleased with this speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "402a1880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary primitively took 7.479 seconds\n",
      "Length of vocabulary is 115238\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "start_time = int(time.time() * 1000)\n",
    "\n",
    "for index, row in train_dfs[args.datasets[0]].iterrows():\n",
    "    for word in row[args.train_column].lower().split():\n",
    "        vocab.add(word)\n",
    "\n",
    "end_time = int(time.time() * 1000)\n",
    "\n",
    "print(f\"Getting vocabulary primitively took {(end_time - start_time) / 1000.0} seconds\")\n",
    "print(f\"Length of vocabulary is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93d2cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary in style took 2.19 seconds\n",
      "Length of vocabulary is 115238\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "start_time = int(time.time() * 1000)\n",
    "\n",
    "train_dfs[args.datasets[0]].apply(lambda row: vocab.update(row[args.train_column].lower().split()), axis = 1)\n",
    "\n",
    "end_time = int(time.time() * 1000)\n",
    "\n",
    "print(f\"Getting vocabulary in style took {(end_time - start_time) / 1000.0} seconds\")\n",
    "print(f\"Length of vocabulary is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8693680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_for_each_dataset(dataframes):\n",
    "    vocabularies = {}\n",
    "    \n",
    "    for dataframe in dataframes:\n",
    "        start_time = int(time.time() * 1000)\n",
    "        \n",
    "        vocab = set()\n",
    "        \n",
    "        df = dataframes[dataframe]\n",
    "        df.apply(lambda row: vocab.update(row[args.train_column].lower().split()), axis = 1)\n",
    "\n",
    "        vocabularies[dataframe] = vocab\n",
    "        \n",
    "        end_time = int(time.time() * 1000)\n",
    "        \n",
    "        print(f\"Getting vocabulary for '{dataframe}' dataset \" \\\n",
    "                f\"took {(end_time - start_time) / 1000.0} seconds, length = {len(vocab)}\")\n",
    "\n",
    "    return vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0841432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vocabulary for 'tokenized' dataset took 2.227 seconds, length = 115238\n",
      "Getting vocabulary for 'filtered' dataset took 2.13 seconds, length = 115219\n",
      "Getting vocabulary for 'lemmatized' dataset took 2.176 seconds, length = 79643\n",
      "Getting vocabulary for 'stemmed' dataset took 2.184 seconds, length = 140792\n",
      "Getting vocabulary for 'lemmatized_filtered' dataset took 2.049 seconds, length = 73616\n",
      "Getting vocabulary for 'stemmed_filtered' dataset took 2.05 seconds, length = 68621\n"
     ]
    }
   ],
   "source": [
    "vocabularies = get_vocabulary_for_each_dataset(train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff5616ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andrológushoz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = list(vocabularies[args.datasets[0]])[4]\n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa662e",
   "metadata": {},
   "source": [
    "#### Getting the index for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9931c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_of_word(vocabularies):\n",
    "    index_of_word = {}\n",
    "    for dataset_key in vocabularies:\n",
    "        index_of_word[dataset_key] = {value: key for key, value in enumerate(vocabularies[dataset_key])}\n",
    "    return index_of_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0588fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_word = create_index_of_word(vocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eea75c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = index_of_word[args.datasets[0]][word]\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293327a",
   "metadata": {},
   "source": [
    "### Get a batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548d12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_rep(question, input_size, index_of_word):\n",
    "    layer = np.zeros(input_size, dtype=float)\n",
    "    words = question.lower().split()\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in index_of_word:\n",
    "            layer[index_of_word[word]] += 1\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3891da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df, i, batch_size, input_size, index_of_word):  \n",
    "    batch = df[i * batch_size : (i + 1) * batch_size]\n",
    "    batch_q = batch[args.train_column].apply(lambda x: bow_rep(x, input_size, index_of_word))\n",
    "    targets_df = batch.main_category.apply(lambda x: target_dict[x])\n",
    "     \n",
    "    return np.array(batch_q.tolist()), np.array(targets_df.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4e2c7",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b370df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "     def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    " \n",
    "     def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3bad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_accuracy(prediction, actual):\n",
    "    rounded_predictions = prediction.argmax(1)\n",
    "    correct = (rounded_predictions == actual).float()\n",
    "\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae7e5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_to_model(model, df, vocab, index_of_word, optimizer, criterion, bar, train):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    total_batch = len(df) // args.batch_size\n",
    "    \n",
    "    total_predicted = []\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_X, batch_y = get_batch(df, i, args.batch_size, len(vocab), index_of_word)\n",
    "        \n",
    "        batch_X = Variable(torch.FloatTensor(batch_X))\n",
    "        batch_y = Variable(torch.LongTensor(batch_y))\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        acc = class_accuracy(predictions, batch_y)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        total_predicted += predicted.tolist()\n",
    "        \n",
    "        bar.set_postfix(loss=(epoch_loss / (i + 1)), acc=(epoch_acc / (i + 1)))\n",
    "        bar.update()\n",
    "\n",
    "    return epoch_loss / total_batch, epoch_acc / total_batch, total_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d017bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, df, vocab, index_of_word, optimizer, criterion, bar):\n",
    "    model.train()\n",
    "    loss, acc, _ = show_data_to_model(model, df, vocab,\\\n",
    "                                      index_of_word, optimizer, criterion, bar, True)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9fb534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, df, vocab, index_of_word, optimizer, criterion, bar):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, acc, predicted = show_data_to_model(model, df, vocab,\\\n",
    "                                      index_of_word, optimizer, criterion, bar, False)\n",
    "        return loss, acc, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e4899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = {}\n",
    "train_accuracies = {}\n",
    "\n",
    "valid_losses = {}\n",
    "valid_accuracies = {}\n",
    "\n",
    "for dataset_key in args.datasets:\n",
    "    start_time = int(time.time() * 1000)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    train_losses[dataset_key] = []\n",
    "    train_accuracies[dataset_key] = []\n",
    "\n",
    "    valid_losses[dataset_key] = []\n",
    "    valid_accuracies[dataset_key] = []\n",
    "    \n",
    "    train_df = train_dfs[dataset_key]\n",
    "    valid_df = valid_dfs[dataset_key]\n",
    "    vocabulary = vocabularies[dataset_key]\n",
    "    index_of_word_ = index_of_word[dataset_key]\n",
    "\n",
    "    model = MLP(len(vocabulary), args.hidden_size, len(target_names))\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n",
    "    \n",
    "    train_size = train_df.shape[0] // args.batch_size\n",
    "    valid_size = valid_df.shape[0] // args.batch_size\n",
    "    \n",
    "    epoch_bar = notebook.tqdm(desc=f\"'{dataset_key}' epoch\", total=args.num_epochs, position=0, leave=False)\n",
    "    train_bar = notebook.tqdm(desc=f\"'{dataset_key}' train\", total=train_size, position=1, leave=False)\n",
    "    valid_bar = notebook.tqdm(desc=f\"'{dataset_key}' valid\",total=valid_size, position=1, leave=False)\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_bar.n = 0\n",
    "        valid_bar.n = 0\n",
    "\n",
    "        train_bar.refresh()\n",
    "        valid_bar.refresh()\n",
    "\n",
    "        train_loss, train_acc = train_model(model, train_df, vocabulary, index_of_word_, optimizer, criterion, train_bar)\n",
    "        valid_loss, valid_acc, _ = evaluate_model(model, valid_df, vocabulary, index_of_word_, optimizer, criterion, valid_bar)\n",
    "    \n",
    "        train_losses[dataset_key].append(train_loss)\n",
    "        train_accuracies[dataset_key].append(train_acc)\n",
    "\n",
    "        valid_losses[dataset_key].append(valid_loss)\n",
    "        valid_accuracies[dataset_key].append(valid_acc)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), args.model_state_file + \"_\" + dataset_key + \".pth\")\n",
    "\n",
    "        epoch_bar.update()\n",
    "        epoch_bar.refresh()\n",
    "    end_time = int(time.time() * 1000)\n",
    "\n",
    "    print(f\"Training took {(end_time - start_time) / 1000.0} seconds for '{dataset_key}' dataset, \" \\\n",
    "        f\"device = '{device}', epochs = {args.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8273196",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(args.datasets), 2, figsize=(10 * 2, 10 * len(args.datasets)))\n",
    "\n",
    "for idx, dataset in enumerate(args.datasets):\n",
    "    axs[idx, 0].set_title(f\"Loss values for '{dataset}'\")\n",
    "    axs[idx, 0].plot([*range(args.num_epochs)], train_losses[dataset], color = 'b', label='Train loss')\n",
    "    axs[idx, 0].plot([*range(args.num_epochs)], valid_losses[dataset], color = 'r', label='Valid loss')\n",
    "    axs[idx, 0].legend(loc=\"upper center\", bbox_to_anchor=(0.0, 1.1), ncol=1)\n",
    "    \n",
    "    axs[idx, 1].set_title(f\"Accuracy values for '{dataset}'\")\n",
    "    axs[idx, 1].axis(ymin = 0, ymax = 1)\n",
    "    axs[idx, 1].plot([*range(args.num_epochs)], train_accuracies[dataset], color = 'b', label='Train accuracy')\n",
    "    axs[idx, 1].plot([*range(args.num_epochs)], valid_accuracies[dataset], color = 'r', label='Valid accuracy')\n",
    "    axs[idx, 1].legend(loc=\"upper center\", bbox_to_anchor=(0.0, 1.1), ncol=1)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a55630",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "number_of_batches = 0\n",
    "\n",
    "for dataset_key in args.datasets:\n",
    "    test_df = test_dfs[dataset_key]\n",
    "   \n",
    "    vocabulary = vocabularies[dataset_key]\n",
    "    index_of_word_ = index_of_word[dataset_key]\n",
    "    \n",
    "    test_size = test_df.shape[0] // args.batch_size\n",
    "    test_bar = notebook.tqdm(desc=f\"'{dataset_key}' test\", total=test_size, position=1, leave=False)\n",
    "\n",
    "    model = MLP(len(vocabulary), args.hidden_size, len(target_names))\n",
    "    model.load_state_dict(torch.load(args.model_state_file + \"_\" + dataset_key + \".pth\"))\n",
    "    _, _, predictions[dataset_key] = evaluate_model(model, test_df, vocabulary, index_of_word_, optimizer, criterion, test_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = test_dfs[args.datasets[0]].main_category.apply(lambda x: target_dict[x]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(11, 5))\n",
    "accuracy_scores = []\n",
    "\n",
    "for dataset in args.datasets:\n",
    "    pred = predictions[dataset]\n",
    "    accuracy_scores.append(accuracy_score(test_target[:len(pred)], pred))\n",
    "ax.axis(ymin = 0.85, ymax = 1)\n",
    "ax.bar(args.datasets, accuracy_scores, width=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(args.datasets) // 2, 2, figsize=(16, 4 * len(args.datasets)))\n",
    "\n",
    "for idx, dataset_key in enumerate(args.datasets):\n",
    "    axs[idx // 2][idx % 2].set_title(f\"Confusion matrix for '{dataset_key}'\")\n",
    "    \n",
    "    pred = predictions[dataset_key]\n",
    "    \n",
    "    cm = confusion_matrix(test_target[:len(pred)], pred)\n",
    "    cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    \n",
    "    heatmap = sn.heatmap(cm_df, annot=True, ax=axs[idx // 2][idx % 2], cmap='Reds', fmt='g', annot_kws={\"size\": 15}, cbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545311e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_key in args.datasets:\n",
    "    pred = predictions[dataset_key]\n",
    "    report = classification_report(test_target[:len(pred)], pred, target_names=target_names)\n",
    "    print(f\"Classification report for '{dataset_key}':\\n{report}\\n{'=' * 60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
