{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b356a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, \\\n",
    "    get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29ed7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    dataset_dir = \".data\",\n",
    "    dataset_prefix = \"faq_with_splits_\",\n",
    "    dataset = \"lemmatized_filtered\",\n",
    "    models = [\"bert-base-multilingual-uncased\", \"SZTAKI-HLT/hubert-base-cc\"],\n",
    "    model_save_dir = \".model_storage/BERT\",\n",
    "    model_state_file = \"model\",\n",
    "    seed = 1234,\n",
    "    num_epochs = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    hidden_size = 100,\n",
    "    batch_size = 16,\n",
    "    cuda = True,\n",
    "    train_column = 'short_question'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1192120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if args.cuda & torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbe683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_state_file = os.path.join(args.model_save_dir, args.model_state_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c586f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".model_storage/BERT directory already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.model_save_dir):\n",
    "    os.makedirs(args.model_save_dir)\n",
    "    print(f\"Created directory {args.model_save_dir}\")\n",
    "else:\n",
    "    print(f\"{args.model_save_dir} directory already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87116c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".data\\faq_with_splits_lemmatized_filtered\n"
     ]
    }
   ],
   "source": [
    "args.dataset = os.path.join(args.dataset_dir, args.dataset_prefix + args.dataset)\n",
    "print(args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f090921",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc17295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf157c5d4104e43865f676fc4e2b11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5080821c9c045329cfa293fd4ff9bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/266k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4c09943e0b4173a8c80ca81adb125a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ba62e949b34a08873c3efef2bf8268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizers = {}\n",
    "\n",
    "for model in args.models:\n",
    "    tokenizers[model] = BertTokenizerFast.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20860c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.dataset + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e01281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_question</th>\n",
       "      <th>long_question</th>\n",
       "      <th>answer</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>megy hasa kutya mi tehet</td>\n",
       "      <td>megy hasa kutya mi tehet körülbelül hét ismerő...</td>\n",
       "      <td>Kutyának kenyeret? Nem semmi lehet az ismerősö...</td>\n",
       "      <td>Állatok</td>\n",
       "      <td>Kutyák</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ti mot szőrös ruha</td>\n",
       "      <td>ti mot szőrös ruha a gép berakott ruha alap kb...</td>\n",
       "      <td>A Furminator nevű fésű egy áldás, annyi szőrt ...</td>\n",
       "      <td>Állatok</td>\n",
       "      <td>Kutyák</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>részleges körülmetélés nehéz gyógyul begyullad...</td>\n",
       "      <td>részleges körülmetélés nehéz gyógyul begyullad...</td>\n",
       "      <td>Szerintem doki vagy ügyelet... Vagy egy baràt ...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Férfiak egészsége</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sos első szemüveg fontos tudnivaló</td>\n",
       "      <td>sos első szemüveg fontos tudnivaló a jobb 0 75...</td>\n",
       "      <td>Semmi köze a szemüveg hordásának vagy nem hord...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Szemproblémák</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>felpuffadt has megszűnik menstruáció összefüggés</td>\n",
       "      <td>felpuffadt has megszűnik menstruáció összefügg...</td>\n",
       "      <td>Elsősorban, amikor feleszméltél, h az anorexia...</td>\n",
       "      <td>Egészség</td>\n",
       "      <td>Nők egészsége</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      short_question  \\\n",
       "0                           megy hasa kutya mi tehet   \n",
       "1                                 ti mot szőrös ruha   \n",
       "2  részleges körülmetélés nehéz gyógyul begyullad...   \n",
       "3                 sos első szemüveg fontos tudnivaló   \n",
       "4   felpuffadt has megszűnik menstruáció összefüggés   \n",
       "\n",
       "                                       long_question  \\\n",
       "0  megy hasa kutya mi tehet körülbelül hét ismerő...   \n",
       "1  ti mot szőrös ruha a gép berakott ruha alap kb...   \n",
       "2  részleges körülmetélés nehéz gyógyul begyullad...   \n",
       "3  sos első szemüveg fontos tudnivaló a jobb 0 75...   \n",
       "4  felpuffadt has megszűnik menstruáció összefügg...   \n",
       "\n",
       "                                              answer main_category  \\\n",
       "0  Kutyának kenyeret? Nem semmi lehet az ismerősö...       Állatok   \n",
       "1  A Furminator nevű fésű egy áldás, annyi szőrt ...       Állatok   \n",
       "2  Szerintem doki vagy ügyelet... Vagy egy baràt ...      Egészség   \n",
       "3  Semmi köze a szemüveg hordásának vagy nem hord...      Egészség   \n",
       "4  Elsősorban, amikor feleszméltél, h az anorexia...      Egészség   \n",
       "\n",
       "        sub_category  split  \n",
       "0             Kutyák    val  \n",
       "1             Kutyák  train  \n",
       "2  Férfiak egészsége  train  \n",
       "3      Szemproblémák  train  \n",
       "4      Nők egészsége  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79052258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['main_category', args.train_column, 'split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb30534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>short_question</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Állatok</td>\n",
       "      <td>megy hasa kutya mi tehet</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Állatok</td>\n",
       "      <td>ti mot szőrös ruha</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Egészség</td>\n",
       "      <td>részleges körülmetélés nehéz gyógyul begyullad...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egészség</td>\n",
       "      <td>sos első szemüveg fontos tudnivaló</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Egészség</td>\n",
       "      <td>felpuffadt has megszűnik menstruáció összefüggés</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  main_category                                     short_question  split\n",
       "0       Állatok                           megy hasa kutya mi tehet    val\n",
       "1       Állatok                                 ti mot szőrös ruha  train\n",
       "2      Egészség  részleges körülmetélés nehéz gyógyul begyullad...  train\n",
       "3      Egészség                 sos első szemüveg fontos tudnivaló  train\n",
       "4      Egészség   felpuffadt has megszűnik menstruáció összefüggés  train"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2408dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Állatok', 'Egészség', 'Szórakozás', 'Számítástechnika']\n",
      "{'Állatok': 0, 'Egészség': 1, 'Szórakozás': 2, 'Számítástechnika': 3}\n"
     ]
    }
   ],
   "source": [
    "target_names = df.main_category.unique().tolist()\n",
    "target_dict = {k: v for v, k in enumerate(target_names)}\n",
    "\n",
    "print(target_names)\n",
    "print(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b78206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.main_category = df.main_category.apply(lambda x: target_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6250b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7474632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        df[args.train_column].tolist(),\n",
    "        max_length = max_seq_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4fdd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df[df.split == 'val']\n",
    "train_df = df[df.split == 'train']\n",
    "test_df = df[df.split == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e10fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = {}\n",
    "\n",
    "for model in args.models:\n",
    "    encoded_data[model] = {}\n",
    "    tokenizer = tokenizers[model]\n",
    "    encoded_data[model]['train'] = tokenize(train_df, tokenizer)\n",
    "    encoded_data[model]['valid'] = tokenize(valid_df, tokenizer)\n",
    "    encoded_data[model]['test'] = tokenize(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e531a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = {}\n",
    "attention_masks = {}\n",
    "\n",
    "for model in args.models:\n",
    "    input_ids[model] = {}\n",
    "    attention_masks[model] = {}\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        input_ids[model][split] = encoded_data[model][split]['input_ids']\n",
    "        attention_masks[model][split] = encoded_data[model][split]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b553bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "labels['train'] = torch.tensor(train_df.main_category.values)\n",
    "labels['valid'] = torch.tensor(valid_df.main_category.values)\n",
    "labels['test'] = torch.tensor(test_df.main_category.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6292889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for model in args.models:\n",
    "    datasets[model] = {}\n",
    "    \n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        datasets[model][split] = TensorDataset(\n",
    "            torch.tensor(input_ids[model][split]),\n",
    "            torch.tensor(attention_masks[model][split]),\n",
    "            labels[split]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d248a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "\n",
    "for model in args.models:\n",
    "    dataloaders[model] = {}\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        dataloaders[model][split] = DataLoader(\n",
    "            datasets[model][split],\n",
    "            sampler=RandomSampler(datasets[model][split]),\n",
    "            batch_size=args.batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29adf744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82005859 0.7529209  1.05188476 1.99306534]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2 3], y=1         0\n",
      "2         1\n",
      "3         1\n",
      "4         1\n",
      "5         0\n",
      "         ..\n",
      "153556    3\n",
      "153557    2\n",
      "153558    3\n",
      "153559    1\n",
      "153560    0\n",
      "Name: main_category, Length: 107490, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "class_wts = compute_class_weight('balanced', np.unique(train_df.main_category), \\\n",
    "                                 train_df.main_category)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57b0c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(class_wts, dtype=torch.float)\n",
    "weights = weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbbad3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_accuracy(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(y_true=labels_flat, y_pred=preds_flat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717b56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_to_model(model, optimizer, criterion, bar, train):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "       \n",
    "    total_predicted = []\n",
    "    idx = 0\n",
    "\n",
    "    for batch in bar:\n",
    "        idx += 1\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids':      batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels':         batch[2],\n",
    "        }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "\n",
    "        loss = criterion(logits, batch[2])\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        acc = class_accuracy(logits, label_ids)\n",
    "\n",
    "        total_predicted.append(logits)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        del batch\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "#        _, predicted = torch.max(predictions.data, 1)\n",
    "#        total_predicted += predicted.tolist()\n",
    "        \n",
    "        bar.set_postfix(loss=(epoch_loss / idx), acc=(epoch_acc / idx))\n",
    "\n",
    "    total_predicted = np.concatenate(total_predicted, axis=0)\n",
    "\n",
    "    return epoch_loss / idx , epoch_acc / idx, total_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37e409b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, bar):\n",
    "    model.train()\n",
    "    loss, acc, _ = show_data_to_model(model, optimizer, criterion, bar, True)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, optimizer, criterion, bar):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss, acc, pred = show_data_to_model(model, optimizer, criterion, bar, False)\n",
    "            \n",
    "    return loss, acc, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23020e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions_val, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # loss = outputs[0]\n",
    "        \n",
    "        logits = outputs[1]\n",
    "        loss = cross_entropy(logits, batch[2])\n",
    "\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions_val.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader) \n",
    "    \n",
    "    predictions_val = np.concatenate(predictions_val, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions_val, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name):\n",
    "    true_vals = None\n",
    "    predictions_val = None\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs[1]\n",
    "            loss = cross_entropy(logits, batch[2])\n",
    "\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "\n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "        val_loss, predictions_val, true_vals = evaluate(dataloader_valid)\n",
    "        val_f1 = f1_score_func(predictions_val, true_vals)\n",
    "        val_bacc = accuracy(predictions_val, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Balanced Accuracy Score: {val_bacc}')\n",
    "\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "    return true_vals, predictions_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = {}\n",
    "train_accuracies = {}\n",
    "\n",
    "valid_losses = {}\n",
    "valid_accuracies = {}\n",
    "\n",
    "for model_ in args.models:\n",
    "    start_time = int(time.time() * 1000)\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    train_losses[model_] = []\n",
    "    train_accuracies[model_] = []\n",
    "\n",
    "    valid_losses[model_] = []\n",
    "    valid_accuracies[model_] = []\n",
    "    \n",
    "    train_dl = dataloaders[model_]['train']\n",
    "    valid_dl = dataloaders[model_]['valid']    \n",
    "        \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_,\n",
    "        num_labels = len(target_names),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False   \n",
    "    )\n",
    "        \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        args.learning_rate,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_dl) * args.num_epochs\n",
    "    )\n",
    "    \n",
    "    criterion = CrossEntropyLoss(weight=weights)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "   \n",
    "    epoch_bar = tqdm(desc=f\"'{model_}' epoch\", total=args.num_epochs, position=0, leave=True)\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_bar = tqdm(train_dl, desc=f\"'{model_}' train\", leave=True)\n",
    "        valid_bar = tqdm(valid_dl, desc=f\"'{model_}' valid\", leave=True)\n",
    "\n",
    "        train_loss, train_acc = train_model(model, optimizer, criterion, train_bar)\n",
    "        valid_loss, valid_acc, _ = evaluate_model(model, optimizer, criterion, valid_bar)\n",
    "\n",
    "        train_losses[model_].append(train_loss)\n",
    "        train_accuracies[model_].append(train_acc)\n",
    "\n",
    "        valid_losses[model_].append(valid_loss)\n",
    "        valid_accuracies[model_].append(valid_acc)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), args.model_state_file + \"_\" + model_ + \".pth\")\n",
    " \n",
    "        epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bab8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, pred = train(model, 'finetuned_BERT.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, predictions, truth = evaluate(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(truth, predictions.argmax(1))\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sn.heatmap(cm_df, annot=True, cmap='Reds', fmt='g', annot_kws={\"size\": 15}, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(truth, predictions.argmax(1), target_names=target_names)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
